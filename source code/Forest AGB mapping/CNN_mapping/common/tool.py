# -*- coding: utf-8 -*-
"""tool.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XtumEffwlb0PTorKCI6m1vn994tiEM9i
"""
from __future__ import division


import logging

import hdf5storage
import numpy as np
import torch
import os
import argparse
import numpy as np
import random
import torch
import torch.nn as nn
import torch.utils.data as udata
import torch.optim as optim
import torchvision.utils as utils

def setup_seed(seed):
     
     np.random.seed(seed)
     random.seed(seed)
     os.environ['PYTHONHASHSEED'] = str(seed)
     torch.manual_seed(seed)
     torch.cuda.manual_seed(seed)
     torch.cuda.manual_seed_all(seed)
     torch.backends.cudnn.deterministic = True

def resume(resume_path,model,optimizer,lr_schedule,p_iter= True,p_lr_s = True):

  if os.path.isfile(resume_path):

    checkpoint = torch.load(resume_path)  # 加载断点

    model.load_state_dict(checkpoint['state_dict'])  # 加载模型可学习参数
    
    start_epoch = checkpoint['epoch']  # 设置开始的epoch

    optimizer.load_state_dict(checkpoint['optimizer'])  # 加载优化器参数

    if p_iter:
      iteration = checkpoint['iter']
      print('exict and load iteration')
      return model,optimizer,start_epoch,iteration

    if p_lr_s:
      lr_schedule.load_state_dict(checkpoint['lr_schedule'])#加载lr_scheduler
      print('exict and load lr_schedule')
      return model,optimizer,start_epoch,lr_schedule

    else:

      return model,optimizer,start_epoch
    
    print("=> loading checkpoint '{}'".format(resume_file))

   
  else:
    print("=> please select loading checkpoint")
          

def save_checkpoint(model_path, epoch, p_iter,p_schedule, iteration, model, optimizer,lr_schedule):
  
  if not os.path.isdir(model_path):
       os.mkdir(model_path)
  
  if p_iter:
    
      state = {
            'epoch': epoch,
            'iter': iteration,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            }
   
  if p_schedule:
      state = {
            'epoch': epoch,
           
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'lr_schedule': lr_schedule.state_dict()
            }
  else:
      state = {
            'epoch': epoch,
           
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            }
    
  torch.save(state, os.path.join(model_path, 'net_%depoch.pth' % epoch))
  

class AverageMeter(object):
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def initialize_logger(file_dir):
    logger = logging.getLogger()
    fhandler = logging.FileHandler(filename=file_dir, mode='a')
    formatter = logging.Formatter('%(asctime)s - %(message)s',"%Y-%m-%d %H:%M:%S")
    fhandler.setFormatter(formatter)
    logger.addHandler(fhandler)
    logger.setLevel(logging.INFO)
    return logger

def record_loss(loss_csv,epoch, iteration, epoch_time, lr, train_loss, test_loss):
    """ Record many results."""
    loss_csv.write('{},{},{},{},{},{}\n'.format(epoch, iteration, epoch_time, lr, train_loss, test_loss))
    loss_csv.flush()    
    loss_csv.close

def record_log(log_path):
    if not os.path.exists(log_path):
      os.makedirs(log_path)
    loss_csv = open(os.path.join(log_path, 'loss.csv'), 'a+')
    log_dir = os.path.join(log_path, 'train.log')
    logger = initialize_logger(log_dir)

    return loss_csv, logger

def load_weight(model,weight_path):
   save_point = torch.load(weight_path)
   model_param = save_point['state_dict']
   model_dict = {}
   for k1, k2 in zip(model.state_dict(), model_param):
      model_dict[k1] = model_param[k2]
   model.load_state_dict(model_dict)
   model = model.cuda()
   model.eval()
   print('Generator is loaded!')
   return model

